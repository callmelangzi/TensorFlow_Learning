#### MAC环境
```python 
# 使用tensorflow 搭建线性回归模型

import numpy as np
import tensorflow as tf
from sklearn.datasets import fetch_california_housing


rng = np.random

X1 = np.array([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,
              7.042,10.791,5.313,7.997,5.654,9.27,3.1])
y1 = np.array([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,
              2.827,3.465,1.65,2.904,2.42,2.94,1.3])


X = tf.constant(X1,dtype = tf.float32, name='X')
y = tf.constant(y1,dtype=tf.float32, name='y')

w = tf.Variable(rng.randn(), name='weight')
b = tf.Variable(rng.randn(), name='bias')


# Linear regression (Wx + b).
def linear_regression(x):
    return w * x + b

# Mean square error.
def mean_square(y_pred, y_true):
    return tf.reduce_mean(tf.square(y_pred - y_true))

# Stochastic Gradient Descent Optimizer.
optimizer = tf.optimizers.SGD(0.01)


def run_optimization():
    # Wrap computation inside a GradientTape for automatic differentiation.
    with tf.GradientTape() as g:
        pred = linear_regression(X)
        loss = mean_square(pred, y)
    # Compute gradients.
    gradients = g.gradient(loss, [w, b])
    # Update W and b following gradients.
    optimizer.apply_gradients(zip(gradients, [w, b]))


for i in range(1,1000):
    run_optimization()
    if i % 100 == 0:
        pred = linear_regression(X)
        err = mean_square(y,pred)
        print("step: %i, loss: %f, W: %f, b: %f" % (i, err, w.numpy(), b.numpy()))

```


整体流程是1.初始化模型参数 2.定义损失函数 3.使用梯度下降法求解
后续优化方向： 小批量梯度下降，所有样本量进行梯度下降时，计算量会较大，相对耗时
