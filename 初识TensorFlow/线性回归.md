#### MAC环境
```python 
# 使用tensorflow 搭建线性回归模型

import numpy as np
import tensorflow as tf
from sklearn.datasets import fetch_california_housing


rng = np.random

X1 = np.array([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,
              7.042,10.791,5.313,7.997,5.654,9.27,3.1])
y1 = np.array([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221,
              2.827,3.465,1.65,2.904,2.42,2.94,1.3])


X = tf.constant(X1,dtype = tf.float32, name='X')
y = tf.constant(y1,dtype=tf.float32, name='y')

w = tf.Variable(rng.randn(), name='weight')
b = tf.Variable(rng.randn(), name='bias')


# Linear regression (Wx + b).
def linear_regression(x):
    return w * x + b

# Mean square error.
def mean_square(y_pred, y_true):
    return tf.reduce_mean(tf.square(y_pred - y_true))

# Stochastic Gradient Descent Optimizer.
optimizer = tf.optimizers.SGD(0.01)


def run_optimization():
    # Wrap computation inside a GradientTape for automatic differentiation.
    with tf.GradientTape() as g:
        pred = linear_regression(X)
        loss = mean_square(pred, y)
    # Compute gradients.
    gradients = g.gradient(loss, [w, b])
    # Update W and b following gradients.
    optimizer.apply_gradients(zip(gradients, [w, b]))


for i in range(1,1000):
    run_optimization()
    if i % 100 == 0:
        pred = linear_regression(X)
        err = mean_square(y,pred)
        print("step: %i, loss: %f, W: %f, b: %f" % (i, err, w.numpy(), b.numpy()))

```


整体流程是1.初始化模型参数 2.定义损失函数 3.使用梯度下降法求解
后续优化方向： 小批量梯度下降，所有样本量进行梯度下降时，计算量会较大，相对耗时



#### Win环境
```python 
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
import numpy as np
from sklearn.datasets import fetch_california_housing
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split


def prepare():
    housing = fetch_california_housing()
    housing_data = housing.data
    scaler = StandardScaler()
    # 数据标准化
    scaler_housing_data = scaler.fit_transform(housing_data)
    X, y =shuffle(scaler_housing_data, housing.target, random_state=42)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
    return X_train, X_test, y_train, y_test, scaler


X_train, X_test, y_train, y_test, scaler = prepare()

X_train_bias = np.c_[np.ones((X_train.shape[0], 1)), X_train]

X = tf.constant(X_train_bias, dtype=tf.float32, name='X')
## reshape 矩阵转换 加快运算速度
y = tf.constant(y_train.reshape(-1,1), dtype=tf.float32, name='y')

theta = tf.Variable(tf.random_uniform([X_train.shape[1] + 1, 1], -1.0, 1), name='theta')
y_pred = tf.matmul(X,theta)
err = y - y_pred
mse = tf.reduce_mean(tf.square(err), name='mse')
optimizer = tf.train.GradientDescentOptimizer(0.01)
loss = optimizer.minimize(mse)


init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    mse_val = sess.run(mse)
    best_params = sess.run(theta)
    for epoch in range(1,100):
        tmp_mse = mse.eval()
        if epoch % 10 == 0:

            print('total loss is', tmp_mse)

        sess.run(loss)
        if mse_val > tmp_mse:
            mse_val = tmp_mse
            best_params = sess.run(theta)


print(best_params)


X_test_bias = np.c_[np.ones((X_test.shape[0], 1)), X_test]
X_test1 = tf.constant(X_test_bias, dtype= tf.float32, name = 'X_test')
y_test = tf.constant(y_test,dtype=tf.float32,name='y_test')
y_pred = tf.matmul(X_test1, best_params)

mse_test = tf.reduce_mean(tf.square( y_test- y_pred))
init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    pred = y_pred.eval()
    print('test err is ',mse_test.eval())

X_test_inverse = scaler.inverse_transform(X_test)

print(np.c_[X_test_inverse, pred])
```


上述代码直接使用了tensorflow的梯度下降optimizer，相对比较简单，也可以自行代码实现
其中对y矩阵进行了reshape(-1,1)操作  明显加快了运行速度
还可以优化的点是: 
* 小批量梯度下降
* 变动的学习率
